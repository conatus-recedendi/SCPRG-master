11/11/2024 12:20:39 - INFO - __main__ -   Checkpoint detected, resuming training at wikievents-base_seed9999/checkpoint-405. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.
11/11/2024 12:20:39 - INFO - __main__ -   hi
11/11/2024 12:20:39 - INFO - __main__ -   True
11/11/2024 12:20:39 - INFO - __main__ -   <module 'torch.version' from '/home/e1_u2/anaconda3/envs/liu2023/lib/python3.9/site-packages/torch/version.py'>
11/11/2024 12:20:39 - INFO - __main__ -   11.7
11/11/2024 12:20:39 - INFO - __main__ -   True
11/11/2024 12:20:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 3distributed training: False, 16-bits training: False
11/11/2024 12:20:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(
_n_gpu=3,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=100,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=wikievents-base_seed9999/runs/Nov11_12-20-39_irrlab,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=f1,
mp_parameters=,
no_cuda=False,
num_train_epochs=100.0,
output_dir=wikievents-base_seed9999,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=wikievents-base_seed9999,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=wikievents-base_seed9999,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=9999,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.1,
)
11/11/2024 12:20:40 - WARNING - datasets.builder -   Using custom data configuration default-3780d20594584047
11/11/2024 12:20:40 - WARNING - datasets.builder -   Reusing dataset text (/home/e1_u2/.cache/huggingface/datasets/text/default-3780d20594584047/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)
11/11/2024 12:20:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5ecdf34670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
